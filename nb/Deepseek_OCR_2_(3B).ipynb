{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_yM3fEuAgI"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://unsloth.ai/docs/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your local device, follow [our guide](https://unsloth.ai/docs/get-started/install). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & how to save it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzDzE2BKuAgL"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2IUvXOnuAgL"
      },
      "source": [
        "Train MoEs - DeepSeek, GLM, Qwen and gpt-oss 12x faster with 35% less VRAM. [Blog](https://unsloth.ai/docs/new/faster-moe)\n",
        "\n",
        "You can now train embedding models 1.8-3.3x faster with 20% less VRAM. [Blog](https://unsloth.ai/docs/new/embedding-finetuning)\n",
        "\n",
        "Ultra Long-Context Reinforcement Learning is here with 7x more context windows! [Blog](https://unsloth.ai/docs/new/grpo-long-context)\n",
        "\n",
        "3x faster LLM training with 30% less VRAM and 500K context. [3x faster](https://unsloth.ai/docs/new/3x-faster-training-packing) ‚Ä¢ [500K Context](https://unsloth.ai/docs/new/500k-context-length-fine-tuning)\n",
        "\n",
        "New in Reinforcement Learning: [FP8 RL](https://unsloth.ai/docs/new/fp8-reinforcement-learning) ‚Ä¢ [Vision RL](https://unsloth.ai/docs/new/vision-reinforcement-learning-vlm-rl) ‚Ä¢ [Standby](https://unsloth.ai/docs/basics/memory-efficient-rl) ‚Ä¢ [gpt-oss RL](https://unsloth.ai/docs/new/gpt-oss-reinforcement-learning)\n",
        "\n",
        "Visit our docs for all our [model uploads](https://unsloth.ai/docs/get-started/unsloth-model-catalog) and [notebooks](https://unsloth.ai/docs/get-started/unsloth-notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2iX-fb0uAgN"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESITUyt2uAgO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth  # Do this in local & cloud setups\n",
        "else:\n",
        "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxLAh3BKuAgQ"
      },
      "source": [
        "Let's prepare the OCR model to our local first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "5f2d5334fcba4369890200a9c2fc3315",
            "dad347615e994e21b475e5d2f4e77050",
            "b5ce46fc35b742fba0a86aced3656c2e",
            "978252c5f1bd4f9eba3692ab2f767440",
            "9f2897dbd12b480fa410b90943e8f02c",
            "7e1c45a962054ce2bc56520cfcc75652",
            "93873bef34d747988bc3beef07855229",
            "04d5d67609654ceb9a10919f8aa2bf53",
            "1129313b2954431aa5f12dc3848cdb1f",
            "7cd7596f6e0b499fa3659bb3731a08b4",
            "0539123ea6e44d37b122fd4f9b0a5ce1"
          ]
        },
        "id": "fbcU2spPuAgQ",
        "outputId": "824ab2b3-a514-4515-87e8-df9815130181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f2d5334fcba4369890200a9c2fc3315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/deepseek_ocr2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR-2\", local_dir = \"deepseek_ocr2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "7fac0e0f-e7e1-4f08-b0cc-bda9ff129d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2026.1.4: Fast Deepseekocr2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n",
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n",
        "    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"./deepseek_ocr2\",\n",
        "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code = True,\n",
        "    unsloth_force_compile = True,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZVh2ByxFNh"
      },
      "source": [
        "### Let's Evaluate Deepseek-OCR Baseline Performance on Persian Transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"hezarai/parsynth-ocr-200k\", split = \"train[:2000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZHjLlxrI_TH"
      },
      "outputs": [],
      "source": [
        "# Save an image that will not be used during training for evaluation purposes\n",
        "dataset[1523]['image_path'].save(\"your_image.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "68O9GTfTXfHo",
        "outputId": "f772564c-0194-4884-ab4f-3f1bac747196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=255x48>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset[1523]['image_path']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpS8-2lHCEOJ",
        "outputId": "8f6e6e36-21ea-4f7c-ce1d-f99af432f236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÿ≠ŸÅÿ∏ ÿßŸÑÿ≥ŸÑÿßŸÖ\n",
            "===============save results:===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image: 0it [00:00, ?it/s]\n",
            "other: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# prompt = \"<image>\\nFree OCR. \"\n",
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "# infer(self, tokenizer, prompt = '', image_file = '', output_path = ' ', base_size = 1024, image_size = 768, crop_mode = True, test_compress = False, save_results = False):\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 768, image_size = 768, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 768, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt = prompt, image_file = image_file, output_path = output_path, base_size = 1024, image_size = 768, crop_mode = True, save_results = True, test_compress = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VTzhtzNRAEL1",
        "outputId": "0e087564-3843-4249-9708-1c8d3436f15a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖŸÖ ÿ¥ÿØŸÖ ÿ≠ŸÇ€åŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€åÿ¥ÿØ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset[1523][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6LYHC3F0Ihf"
      },
      "source": [
        "<h3>Baseline Model Performance: 23% Character Error Rate (CER) for this sample !</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzckMII_02s_"
      },
      "source": [
        "# Let's finetune Deepseek-OCR !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "aa51dc56-23f8-418b-ea6d-e4fa31cdb92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We'll be using a dataset for Persian OCR. The goal is to convert these images into a computer readable form - ie text. This can be very useful for digitizing Persian text.\n",
        "\n",
        "You can access the dataset [here](https://huggingface.co/datasets/hezarai/parsynth-ocr-200k)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"<|User|>\",\n",
        "  \"content\": \"\",\n",
        "  \"images\": []\n",
        "},\n",
        "{ \"role\": \"<|Assistant|>\",\n",
        "  \"content\": \"\"\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "instruction = \"<image>\\nFree OCR. \"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    \"\"\"Convert dataset sample to conversation format\"\"\"\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"<|User|>\",\n",
        "            \"content\": instruction,\n",
        "            \"images\": [sample['image']]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"<|Assistant|>\",\n",
        "            \"content\": sample[\"text\"]\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"hezarai/parsynth-ocr-200k\", split = \"train[:1000]\")\n",
        "dataset = dataset.rename_column(\"image_path\", \"image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-9u-OD6_gE"
      },
      "source": [
        "Let's convert the dataset into the \"correct\" format for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLHM0PGn30x5"
      },
      "outputs": [],
      "source": [
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "2ee7dd17-f2d8-4bf7-c6c8-85be675737a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': '<|User|>',\n",
              "   'content': '<image>\\nFree OCR. ',\n",
              "   'images': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=218x48>]},\n",
              "  {'role': '<|Assistant|>', 'content': 'ŸáŸÖŸáÿßÿ¥ ÿ¨ÿ®ÿ±Ÿá Ÿà ÿßÿÆÿ™€åÿßÿ± ÿ™ŸàŸáŸÖŸá'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "converted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E2WR-p20LcG_"
      },
      "outputs": [],
      "source": [
        "# @title Create datacollator\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from PIL import Image, ImageOps\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import io\n",
        "\n",
        "from deepseek_ocr2.modeling_deepseekocr2 import (\n",
        "    format_messages,\n",
        "    text_encode,\n",
        "    BasicImageTransform,\n",
        "    dynamic_preprocess,\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class DeepSeekOCR2DataCollator:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tokenizer: Tokenizer\n",
        "        model: Model\n",
        "        image_size: Size for image patches (default: 768)\n",
        "        base_size: Size for global view (default: 1024)\n",
        "        crop_mode: Whether to use dynamic cropping for large images\n",
        "        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n",
        "    \"\"\"\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    image_size: int = 768\n",
        "    base_size: int = 1024\n",
        "    crop_mode: bool = True\n",
        "    image_token_id: int = 128815\n",
        "    train_on_responses_only: bool = True\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_size: int = 768,\n",
        "        base_size: int = 1024,\n",
        "        crop_mode: bool = True,\n",
        "        train_on_responses_only: bool = True,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.crop_mode = crop_mode\n",
        "        self.image_token_id = 128815\n",
        "        self.dtype = model.dtype  # Get dtype from model\n",
        "        self.train_on_responses_only = train_on_responses_only\n",
        "\n",
        "        self.image_transform = BasicImageTransform(\n",
        "            mean = (0.5, 0.5, 0.5),\n",
        "            std = (0.5, 0.5, 0.5),\n",
        "            normalize = True\n",
        "        )\n",
        "        self.patch_size = 16\n",
        "        self.downsample_ratio = 4\n",
        "\n",
        "        # Get BOS token ID from tokenizer\n",
        "        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n",
        "            self.bos_id = tokenizer.bos_token_id\n",
        "        else:\n",
        "            self.bos_id = 0\n",
        "            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n",
        "\n",
        "    def deserialize_image(self, image_data) -> Image.Image:\n",
        "        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n",
        "        if isinstance(image_data, Image.Image):\n",
        "            return image_data.convert(\"RGB\")\n",
        "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "            image_bytes = image_data['bytes']\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            return image.convert(\"RGB\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
        "\n",
        "    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n",
        "        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n",
        "        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "        width_crop_num, height_crop_num = crop_ratio\n",
        "\n",
        "        if self.crop_mode:\n",
        "            img_tokens = num_queries_base * num_queries_base + 1\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                img_tokens += (num_queries * width_crop_num) * (num_queries * height_crop_num)\n",
        "        else:\n",
        "            img_tokens = num_queries * num_queries + 1\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Process a single image based on crop_mode and size thresholds\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n",
        "        \"\"\"\n",
        "        images_list = []\n",
        "        images_crop_list = []\n",
        "        images_spatial_crop = []\n",
        "\n",
        "        if self.crop_mode:\n",
        "            # Determine crop ratio based on image size\n",
        "            if image.size[0] <= 768 and image.size[1] <= 768:\n",
        "                crop_ratio = (1, 1)\n",
        "                images_crop_raw = []\n",
        "            else:\n",
        "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
        "                    image, min_num = 2, max_num = 6,\n",
        "                    image_size = self.image_size, use_thumbnail = False\n",
        "                )\n",
        "\n",
        "            # Process global view with padding\n",
        "            global_view = ImageOps.pad(\n",
        "                image, (self.base_size, self.base_size),\n",
        "                color = tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "            )\n",
        "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            width_crop_num, height_crop_num = crop_ratio\n",
        "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "\n",
        "            # Process local views (crops) if applicable\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                for crop_img in images_crop_raw:\n",
        "                    images_crop_list.append(\n",
        "                        self.image_transform(crop_img).to(self.dtype)\n",
        "                    )\n",
        "\n",
        "            # Calculate image tokens\n",
        "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "            tokenized_image = ([self.image_token_id] * num_queries_base) * num_queries_base\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num)) * (\n",
        "                    num_queries * height_crop_num)\n",
        "\n",
        "        else:  # crop_mode = False\n",
        "            crop_ratio = (1, 1)\n",
        "            images_spatial_crop.append([1, 1])\n",
        "\n",
        "            # For smaller base sizes, resize; for larger, pad\n",
        "            if self.base_size <= 768:\n",
        "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
        "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
        "            else:\n",
        "                global_view = ImageOps.pad(\n",
        "                    image, (self.base_size, self.base_size),\n",
        "                    color = tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                )\n",
        "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "            tokenized_image = ([self.image_token_id] * num_queries) * num_queries\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
        "\n",
        "    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single conversation into model inputs.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Setup ---\n",
        "        images = []\n",
        "        for message in messages:\n",
        "            if \"images\" in message and message[\"images\"]:\n",
        "                for img_data in message[\"images\"]:\n",
        "                    if img_data is not None:\n",
        "                        pil_image = self.deserialize_image(img_data)\n",
        "                        images.append(pil_image)\n",
        "\n",
        "        if not images:\n",
        "            raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n",
        "\n",
        "        tokenized_str = []\n",
        "        images_seq_mask = []\n",
        "        images_list, images_crop_list, images_spatial_crop = [], [], []\n",
        "\n",
        "        prompt_token_count = -1 # Index to start training\n",
        "        assistant_started = False\n",
        "        image_idx = 0\n",
        "\n",
        "        # Add BOS token at the very beginning\n",
        "        tokenized_str.append(self.bos_id)\n",
        "        images_seq_mask.append(False)\n",
        "\n",
        "        for message in messages:\n",
        "            role = message[\"role\"]\n",
        "            content = message[\"content\"]\n",
        "\n",
        "            # Check if this is the assistant's turn\n",
        "            if role == \"<|Assistant|>\":\n",
        "                if not assistant_started:\n",
        "                    # This is the split point. All tokens added *so far*\n",
        "                    # are part of the prompt.\n",
        "                    prompt_token_count = len(tokenized_str)\n",
        "                    assistant_started = True\n",
        "\n",
        "                # Append the EOS token string to the *end* of assistant content\n",
        "                content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
        "\n",
        "            # Split this message's content by the image token\n",
        "            text_splits = content.split('<image>')\n",
        "\n",
        "            for i, text_sep in enumerate(text_splits):\n",
        "                # Tokenize the text part\n",
        "                tokenized_sep = text_encode(self.tokenizer, text_sep, bos = False, eos = False)\n",
        "                tokenized_str.extend(tokenized_sep)\n",
        "                images_seq_mask.extend([False] * len(tokenized_sep))\n",
        "\n",
        "                # If this text is followed by an <image> tag\n",
        "                if i < len(text_splits) - 1:\n",
        "                    if image_idx >= len(images):\n",
        "                        raise ValueError(\n",
        "                            f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n",
        "                        )\n",
        "\n",
        "                    # Process the image\n",
        "                    image = images[image_idx]\n",
        "                    img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n",
        "\n",
        "                    images_list.extend(img_list)\n",
        "                    images_crop_list.extend(crop_list)\n",
        "                    images_spatial_crop.extend(spatial_crop)\n",
        "\n",
        "                    # Add image placeholder tokens\n",
        "                    tokenized_str.extend(tok_img)\n",
        "                    images_seq_mask.extend([True] * len(tok_img))\n",
        "\n",
        "                    image_idx += 1 # Move to the next image\n",
        "\n",
        "        # --- 3. Validation and Final Prep ---\n",
        "        if image_idx != len(images):\n",
        "            raise ValueError(\n",
        "                f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n",
        "            )\n",
        "\n",
        "        # If we never found an assistant message, we're in a weird state\n",
        "        # (e.g., user-only prompt). We mask everything.\n",
        "        if not assistant_started:\n",
        "            print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n",
        "            prompt_token_count = len(tokenized_str)\n",
        "\n",
        "        # Prepare image tensors\n",
        "        images_ori = torch.stack(images_list, dim = 0)\n",
        "        images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype = torch.long)\n",
        "\n",
        "        if images_crop_list:\n",
        "            images_crop = torch.stack(images_crop_list, dim = 0)\n",
        "        else:\n",
        "            images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype = self.dtype)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(tokenized_str, dtype = torch.long),\n",
        "            \"images_seq_mask\": torch.tensor(images_seq_mask, dtype = torch.bool),\n",
        "            \"images_ori\": images_ori,\n",
        "            \"images_crop\": images_crop,\n",
        "            \"images_spatial_crop\": images_spatial_crop_tensor,\n",
        "            \"prompt_token_count\": prompt_token_count, # This is now accurate\n",
        "        }\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of samples\"\"\"\n",
        "        batch_data = []\n",
        "\n",
        "        # Process each sample\n",
        "        for feature in features:\n",
        "            try:\n",
        "                processed = self.process_single_sample(feature['messages'])\n",
        "                batch_data.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_data:\n",
        "            raise ValueError(\"No valid samples in batch\")\n",
        "\n",
        "        # Extract lists\n",
        "        input_ids_list = [item['input_ids'] for item in batch_data]\n",
        "        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n",
        "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids_list, batch_first = True, padding_value = self.tokenizer.pad_token_id)\n",
        "        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first = True, padding_value = False)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens (model shouldn't predict these)\n",
        "        labels[images_seq_mask] = -100\n",
        "\n",
        "        # Mask user prompt tokens when train_on_responses_only = True (only train on assistant responses)\n",
        "        if self.train_on_responses_only:\n",
        "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
        "                if prompt_count > 0:\n",
        "                    labels[idx, :prompt_count] = -100\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        # Prepare images batch (list of tuples)\n",
        "        images_batch = []\n",
        "        for item in batch_data:\n",
        "            images_batch.append((item['images_crop'], item['images_ori']))\n",
        "\n",
        "        # Stack spatial crop info\n",
        "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim = 0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask,\n",
        "            \"images_spatial_crop\": images_spatial_crop,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support `DPOTrainer` and `GRPOTrainer` for reinforcement learning!!\n",
        "\n",
        "We use our new `DeepSeekOCR2DataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "fee3da10-a019-4d0f-c5e6-9111b4d00cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1258209068.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from unsloth import is_bf16_supported\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "data_collator = DeepSeekOCR2DataCollator(\n",
        "    tokenizer = tokenizer,\n",
        "    model = model,\n",
        "    image_size = 768,\n",
        "    base_size = 1024,\n",
        "    crop_mode = True,\n",
        "    train_on_responses_only = True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator, # Must use!\n",
        "    train_dataset = converted_dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        fp16 = not is_bf16_supported(),  # Use fp16 if bf16 is not supported\n",
        "        bf16 = is_bf16_supported(),  # Use bf16 if supported\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "        dataloader_num_workers = 2,\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "6650b521-d4ff-4c1b-919b-c9405e3d628a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.838 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "faeea13b-8e51-4eb1-8be6-aa4b455a7ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 86,307,840 of 3,475,427,200 (2.48% trained)\n",
            "Unsloth: Not an error, but DeepseekOCR2ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 12:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.732300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.539100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.967000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.932800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>4.107200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.477200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.603300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.790600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.453700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.124800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.913800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.877100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.519500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.734100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.735800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.283800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.083100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.348600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.386600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.965000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.822000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.386700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.161900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.866200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.854100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.190900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.507900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.719900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.374100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.871500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.992500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.233700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.457100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.160900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.720300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.266300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.966600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.738700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.535800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.858200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5112776-a367-4d90-fa72-4b50847c1e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "744.5682 seconds used for training.\n",
            "12.41 minutes used for training.\n",
            "Peak reserved memory = 8.367 GB.\n",
            "Peak reserved memory for training = 1.529 GB.\n",
            "Peak reserved memory % of max memory = 56.76 %.\n",
            "Peak reserved memory for training % of max memory = 10.372 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "035e1388-6e76-49ce-ef7e-7b5be6a80305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖ ÿ¥ÿØŸÖ ÿ≠ŸÇ€åŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€åÿ¥Ÿá\n",
            "===============save results:===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image: 0it [00:00, ?it/s]\n",
            "other: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 768, image_size = 768, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 768, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt = prompt, image_file = image_file,\n",
        "    output_path = output_path,\n",
        "    image_size = 768,\n",
        "    base_size = 1024,\n",
        "    crop_mode = True,\n",
        "    save_results = True,\n",
        "    test_compress = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd30-Yg3fEeK"
      },
      "source": [
        "With only 60 steps, we dramatically improved the transcription quality. The Character Error Rate (CER) on this single sample dropped from 23% to 6%, a 74% relative reduction!\n",
        "\n",
        "| Type | OCR |\n",
        "| :--- | :--- |\n",
        "| **Baseline (Pre-Finetune)** | `ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖ ŸÜŸÜÿØŸÖ ÿ≠ŸÇŸäŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖŸäÿ™ŸÜÿØ` |\n",
        "| **Finetuned (60 steps)** | `ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖ ŸÜÿ¥ÿØŸÖ ÿ≠ŸÇ€åŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€åÿ¥ÿØ` |\n",
        "| **Ground Truth** | `ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖŸÖ ÿ¥ÿØŸÖ ÿ≠ŸÇ€åŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€åÿ¥ÿØ` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Hugging Face's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "16d8773b-b970-4ba7-fc10-8416ba95e382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.save_pretrained(\"deepseek_ocr_lora\")  # Local saving\n",
        "tokenizer.save_pretrained(\"deepseek_ocr_lora\")\n",
        "# model.push_to_hub(\"your_name/deepseek_ocr_lora\", token = \"YOUR_HF_TOKEN\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/deepseek_ocr_lora\", token = \"YOUR_HF_TOKEN\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a236f2c-94d4-48dc-f5a1-c45ad89796c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÿßŸÜÿ∂ÿ®ÿßÿ∑ŸÖ ÿ¥ÿØŸÖ ÿ≠ŸÇ€åŸÇÿ™ŸÜ ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€åÿ¥Ÿá\n",
            "===============save results:===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image: 0it [00:00, ?it/s]\n",
            "other: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        model_name = \"deepseek_ocr_lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "        auto_model = AutoModel,\n",
        "        trust_remote_code = True,\n",
        "        unsloth_force_compile = True,\n",
        "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 768, image_size = 768, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 768, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt = prompt, image_file = image_file,\n",
        "    output_path = output_path,\n",
        "    image_size = 768,\n",
        "    base_size = 1024,\n",
        "    crop_mode = True,\n",
        "    save_results = True,\n",
        "    test_compress = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens. See [our docs](https://unsloth.ai/docs/basics/inference-and-deployment) for more deployment options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"YOUR_HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFGPyoM1uAgl"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other resources:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "4. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://unsloth.ai/docs/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://unsloth.ai/docs/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f2d5334fcba4369890200a9c2fc3315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dad347615e994e21b475e5d2f4e77050",
              "IPY_MODEL_b5ce46fc35b742fba0a86aced3656c2e",
              "IPY_MODEL_978252c5f1bd4f9eba3692ab2f767440"
            ],
            "layout": "IPY_MODEL_9f2897dbd12b480fa410b90943e8f02c"
          }
        },
        "dad347615e994e21b475e5d2f4e77050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e1c45a962054ce2bc56520cfcc75652",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_93873bef34d747988bc3beef07855229",
            "value": "Fetching‚Äá16‚Äáfiles:‚Äá100%"
          }
        },
        "b5ce46fc35b742fba0a86aced3656c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04d5d67609654ceb9a10919f8aa2bf53",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1129313b2954431aa5f12dc3848cdb1f",
            "value": 16
          }
        },
        "978252c5f1bd4f9eba3692ab2f767440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd7596f6e0b499fa3659bb3731a08b4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0539123ea6e44d37b122fd4f9b0a5ce1",
            "value": "‚Äá16/16‚Äá[00:00&lt;00:00,‚Äá789.63it/s]"
          }
        },
        "9f2897dbd12b480fa410b90943e8f02c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1c45a962054ce2bc56520cfcc75652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93873bef34d747988bc3beef07855229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04d5d67609654ceb9a10919f8aa2bf53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1129313b2954431aa5f12dc3848cdb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cd7596f6e0b499fa3659bb3731a08b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0539123ea6e44d37b122fd4f9b0a5ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}